{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLK9o4VKQDMNdw1KEQqghA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divya-r-kamat/MnistLite-8k/blob/main/MnistLite_8k_modelrun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNist Neural Network\n",
        "\n",
        "The goal is to achieve 99.4% validation/test accuracy consistently, with less than 15 epochs and 8k parameters\n",
        "\n"
      ],
      "metadata": {
        "id": "hX9pHnOLkK53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/MnistLite-8k/"
      ],
      "metadata": {
        "id": "WHZTZLp_iWjE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qt4kW1wilnj",
        "outputId": "0a955348-c470-46bf-ceda-09ee351507e7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/divya-r-kamat/MnistLite-8k.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxYYEJjfdwbP",
        "outputId": "0b9f50f6-6847-4eba-e182-5b790d14646a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MnistLite-8k'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 45 (delta 21), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (45/45), 340.50 KiB | 1.84 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MnistLite-8k/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhBXtE-Tl9Lt",
        "outputId": "8b1c034a-f502-46c0-d946-929cbb073172"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MnistLite-8k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1\n",
        "\n",
        "### Target:\n",
        "- Get the set-up right\n",
        "- Set Transforms\n",
        "- Set Data Loader\n",
        "- Set Basic Working Code\n",
        "- Set Basic Training & Test Loop\n",
        "- Get the basic skeleton right\n",
        "### Results:\n",
        "- Parameters: 26.5k\n",
        "- Best Training Accuracy: 99.64%\n",
        "- Best Test Accuracy: 99.13%\n",
        "### Analysis\n",
        "- In the initial epochs, the model quickly jumps from ~62% test accuracy (Epoch 1) to ~97.74% (Epoch 2) and ~98%+ by Epoch 3–4. This shows the network is learning MNIST features very efficiently even with a small parameter count.\n",
        "- Training accuracy steadily rises and exceeds 99% around Epoch 10, while test accuracy stabilizes between 98.8–99.1%.\n",
        "- No major overfitting observed — the gap between training and test accuracy remains within ~0.5%, which is acceptable.\n",
        "- The model is relatively lightweight (26.5k params) compared to typical CNNs on MNIST, yet it achieves strong performance close to larger networks.\n",
        "Slight oscillations in test accuracy after Epoch 10 (e.g., 99.06 → 98.85 → 98.97 → 99.13) are normal variance, not a sign of instability.\n",
        "- One-off spike in training loss at Epoch 8 (0.2567) despite high accuracy could be due to noisy batch or optimizer behavior.\n",
        "E- arly training shows very fast convergence. Performance plateaus around 99%, suggesting further gains may require architectural changes or regularization."
      ],
      "metadata": {
        "id": "pLa8K91Gdo2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python train.py --model model1.py --optimizer optimizer --scheduler scheduler --train_transforms train_transforms --test_transforms test_transforms\n",
        "\n",
        "# !python train.py \\\n",
        "#   --model model1.py \\\n",
        "#   --optimizer \"optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\" \\\n",
        "#   --scheduler \"optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\" \\\n",
        "#   --train_transforms train_transforms \\\n",
        "#   --test_transforms test_transforms\n",
        "\n",
        "\n",
        "!python train.py \\\n",
        "  --model model1.py \\\n",
        "  --optimizer \"optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\" \\\n",
        "  --scheduler \"optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\" \\\n",
        "  --train_transforms \"transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\" \\\n",
        "  --test_transforms \"transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYEZNlNPbMTx",
        "outputId": "f91b2946-068a-4ea2-be90-0e5dc869fffe"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded model class 'Net' from model1.py\n",
            "CUDA Available? True\n",
            "\n",
            "Download the train and test dataset, with transforms....\n",
            "100% 9.91M/9.91M [00:00<00:00, 20.6MB/s]\n",
            "100% 28.9k/28.9k [00:00<00:00, 491kB/s]\n",
            "100% 1.65M/1.65M [00:00<00:00, 4.61MB/s]\n",
            "100% 4.54k/4.54k [00:00<00:00, 15.2MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\n",
            "Model Training....\n",
            "\n",
            "Epoch 1\n",
            "Train loss=0.1072 batch_id=468 Accuracy=62.38: 100% 469/469 [00:12<00:00, 38.92it/s]\n",
            "Test set: Average loss: 0.1845, Accuracy: 9393/10000 (93.93%)\n",
            "\n",
            "Epoch 2\n",
            "Train loss=0.1953 batch_id=468 Accuracy=96.42: 100% 469/469 [00:12<00:00, 38.63it/s]\n",
            "Test set: Average loss: 0.0693, Accuracy: 9774/10000 (97.74%)\n",
            "\n",
            "Epoch 3\n",
            "Train loss=0.0355 batch_id=468 Accuracy=97.80: 100% 469/469 [00:12<00:00, 38.51it/s]\n",
            "Test set: Average loss: 0.0575, Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch 4\n",
            "Train loss=0.0486 batch_id=468 Accuracy=98.38: 100% 469/469 [00:12<00:00, 38.97it/s]\n",
            "Test set: Average loss: 0.0494, Accuracy: 9838/10000 (98.38%)\n",
            "\n",
            "Epoch 5\n",
            "Train loss=0.0223 batch_id=468 Accuracy=98.69: 100% 469/469 [00:11<00:00, 41.22it/s]\n",
            "Test set: Average loss: 0.0397, Accuracy: 9871/10000 (98.71%)\n",
            "\n",
            "Epoch 6\n",
            "Train loss=0.0085 batch_id=468 Accuracy=98.92: 100% 469/469 [00:12<00:00, 38.89it/s]\n",
            "Test set: Average loss: 0.0430, Accuracy: 9866/10000 (98.66%)\n",
            "\n",
            "Epoch 7\n",
            "Train loss=0.0446 batch_id=468 Accuracy=99.05: 100% 469/469 [00:12<00:00, 38.87it/s]\n",
            "Test set: Average loss: 0.0338, Accuracy: 9887/10000 (98.87%)\n",
            "\n",
            "Epoch 8\n",
            "Train loss=0.2567 batch_id=468 Accuracy=99.12: 100% 469/469 [00:12<00:00, 38.64it/s]\n",
            "Test set: Average loss: 0.0311, Accuracy: 9906/10000 (99.06%)\n",
            "\n",
            "Epoch 9\n",
            "Train loss=0.0181 batch_id=468 Accuracy=99.28: 100% 469/469 [00:12<00:00, 39.08it/s]\n",
            "Test set: Average loss: 0.0388, Accuracy: 9885/10000 (98.85%)\n",
            "\n",
            "Epoch 10\n",
            "Train loss=0.0428 batch_id=468 Accuracy=99.28: 100% 469/469 [00:12<00:00, 38.84it/s]\n",
            "Test set: Average loss: 0.0310, Accuracy: 9897/10000 (98.97%)\n",
            "\n",
            "Epoch 11\n",
            "Train loss=0.0022 batch_id=468 Accuracy=99.39: 100% 469/469 [00:12<00:00, 38.89it/s]\n",
            "Test set: Average loss: 0.0309, Accuracy: 9912/10000 (99.12%)\n",
            "\n",
            "Epoch 12\n",
            "Train loss=0.0033 batch_id=468 Accuracy=99.44: 100% 469/469 [00:12<00:00, 38.32it/s]\n",
            "Test set: Average loss: 0.0324, Accuracy: 9902/10000 (99.02%)\n",
            "\n",
            "Epoch 13\n",
            "Train loss=0.0004 batch_id=468 Accuracy=99.49: 100% 469/469 [00:12<00:00, 38.82it/s]\n",
            "Test set: Average loss: 0.0310, Accuracy: 9908/10000 (99.08%)\n",
            "\n",
            "Epoch 14\n",
            "Train loss=0.0033 batch_id=468 Accuracy=99.59: 100% 469/469 [00:11<00:00, 39.25it/s]\n",
            "Test set: Average loss: 0.0362, Accuracy: 9904/10000 (99.04%)\n",
            "\n",
            "Epoch 15\n",
            "Train loss=0.0147 batch_id=468 Accuracy=99.58: 100% 469/469 [00:12<00:00, 38.86it/s]\n",
            "Test set: Average loss: 0.0380, Accuracy: 9913/10000 (99.13%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model2\n",
        "\n",
        "### Target:\n",
        "- Make the model lighter (reduce parameter count, stay < 8k)\n",
        "- Use Global Average Pooling (GAP) → reduces overfitting, removes need for large dense layer\n",
        "- Add BatchNorm after convs → stabilize training, faster convergence, better generalization\n",
        "### Results:\n",
        "- Parameters: 5.6k\n",
        "- Best Training Accuracy: 99.57%\n",
        "- Best Test Accuracy: 99.13%\n",
        "### Analysis\n",
        "- lighter model + GAP + BN gave better stability, fewer params, and accuracy that scales above 99% by 13–15 epochs.\n",
        "- still far of the 99.4% milestone (peaked at ~99.13%).\n",
        "- BatchNorm and GAP contribute to stable training and reduced overfitting — the test accuracy remains close to training accuracy throughout..\n",
        "- However, the goal of consistently hitting 99.4% is not yet met — performance plateaus around 99.1–99.2%.."
      ],
      "metadata": {
        "id": "eW0q7ERCkd8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "  --model model2.py \\\n",
        "  --optimizer \"optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\" \\\n",
        "  --scheduler \"optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\" \\\n",
        "  --train_transforms \"transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\" \\\n",
        "  --test_transforms \"transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1JC7Xk3hNU1",
        "outputId": "7abc5ea4-b584-4742-afad-6e80be5983e7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded model class 'Net' from model2.py\n",
            "CUDA Available? True\n",
            "\n",
            "Download the train and test dataset, with transforms....\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\n",
            "Model Training....\n",
            "\n",
            "Epoch 1\n",
            "Train loss=0.0637 batch_id=468 Accuracy=91.11: 100% 469/469 [00:12<00:00, 37.89it/s]\n",
            "Test set: Average loss: 0.0782, Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch 2\n",
            "Train loss=0.0363 batch_id=468 Accuracy=98.12: 100% 469/469 [00:12<00:00, 38.95it/s]\n",
            "Test set: Average loss: 0.0612, Accuracy: 9843/10000 (98.43%)\n",
            "\n",
            "Epoch 3\n",
            "Train loss=0.0202 batch_id=468 Accuracy=98.53: 100% 469/469 [00:12<00:00, 38.82it/s]\n",
            "Test set: Average loss: 0.0428, Accuracy: 9861/10000 (98.61%)\n",
            "\n",
            "Epoch 4\n",
            "Train loss=0.0086 batch_id=468 Accuracy=98.81: 100% 469/469 [00:12<00:00, 38.22it/s]\n",
            "Test set: Average loss: 0.0356, Accuracy: 9885/10000 (98.85%)\n",
            "\n",
            "Epoch 5\n",
            "Train loss=0.0729 batch_id=468 Accuracy=98.94: 100% 469/469 [00:12<00:00, 38.02it/s]\n",
            "Test set: Average loss: 0.0353, Accuracy: 9885/10000 (98.85%)\n",
            "\n",
            "Epoch 6\n",
            "Train loss=0.0816 batch_id=468 Accuracy=99.08: 100% 469/469 [00:12<00:00, 37.20it/s]\n",
            "Test set: Average loss: 0.0423, Accuracy: 9870/10000 (98.70%)\n",
            "\n",
            "Epoch 7\n",
            "Train loss=0.0048 batch_id=468 Accuracy=99.18: 100% 469/469 [00:12<00:00, 37.99it/s]\n",
            "Test set: Average loss: 0.0341, Accuracy: 9886/10000 (98.86%)\n",
            "\n",
            "Epoch 8\n",
            "Train loss=0.0203 batch_id=468 Accuracy=99.19: 100% 469/469 [00:12<00:00, 37.81it/s]\n",
            "Test set: Average loss: 0.0334, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "Epoch 9\n",
            "Train loss=0.0063 batch_id=468 Accuracy=99.31: 100% 469/469 [00:12<00:00, 38.32it/s]\n",
            "Test set: Average loss: 0.0290, Accuracy: 9908/10000 (99.08%)\n",
            "\n",
            "Epoch 10\n",
            "Train loss=0.0046 batch_id=468 Accuracy=99.39: 100% 469/469 [00:11<00:00, 39.16it/s]\n",
            "Test set: Average loss: 0.0317, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "Epoch 11\n",
            "Train loss=0.0148 batch_id=468 Accuracy=99.42: 100% 469/469 [00:12<00:00, 38.17it/s]\n",
            "Test set: Average loss: 0.0315, Accuracy: 9894/10000 (98.94%)\n",
            "\n",
            "Epoch 12\n",
            "Train loss=0.0436 batch_id=468 Accuracy=99.48: 100% 469/469 [00:12<00:00, 37.82it/s]\n",
            "Test set: Average loss: 0.0294, Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "Epoch 13\n",
            "Train loss=0.0280 batch_id=468 Accuracy=99.50: 100% 469/469 [00:12<00:00, 37.83it/s]\n",
            "Test set: Average loss: 0.0255, Accuracy: 9913/10000 (99.13%)\n",
            "\n",
            "Epoch 14\n",
            "Train loss=0.0367 batch_id=468 Accuracy=99.50: 100% 469/469 [00:12<00:00, 38.01it/s]\n",
            "Test set: Average loss: 0.0326, Accuracy: 9898/10000 (98.98%)\n",
            "\n",
            "Epoch 15\n",
            "Train loss=0.0275 batch_id=468 Accuracy=99.57: 100% 469/469 [00:12<00:00, 37.50it/s]\n",
            "Test set: Average loss: 0.0298, Accuracy: 9906/10000 (99.06%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model3\n",
        "\n",
        "### Target:\n",
        "\n",
        "- Add Dropout to reduce overfitting and improve generalization\n",
        "- Increase parameter count from ~5k to ~7.8k for better capacity\n",
        "- Use CosineAnnealingLR scheduler for smoother learning rate decay\n",
        "- Use SGD with Nesterov momentum and weight decay for better convergence\n",
        "- Apply data augmentation: RandomRotation, RandomAffine, Shear, Translation\n",
        "- Retain BatchNorm and GAP for stable training and compact outputgeneralization\n",
        "### Results:\n",
        "- Parameters: 7,864\n",
        "- Best Training Accuracy: 99.36%\n",
        "- Best Test Accuracy: 99.42%\n",
        "- Epochs Run: 15\n",
        "- Achieved 99.4%+ test accuracy consistently by Epoch 11–15\n",
        "### Analysis\n",
        "- The model shows strong early performance, reaching 98.27% test accuracy in Epoch 1 and 99.16% by Epoch 3, indicating fast and stable convergence.\n",
        "- Dropout layers help maintain generalization, with minimal overfitting observed — test accuracy closely tracks training accuracy throughout.\n",
        "- CosineAnnealingLR scheduler contributes to smooth learning rate decay, helping the model avoid sharp drops or spikes in performance.\n",
        "- SGD with Nesterov momentum and weight decay improves optimization dynamics, especially in later epochs.\n",
        "- Data augmentation adds robustness, helping the model generalize better to unseen data and pushing test accuracy past 99.4%.\n",
        "- The model consistently hits 99.4%+ test accuracy from Epoch 9 onward, meeting the target goal within 15 epochs.\n",
        "- Despite increasing parameters to ~7.8k, the model remains lightweight and efficient, balancing capacity and generalization well.\n",
        "- Final test accuracy of 99.42% with stable loss (~0.018) confirms that the model is well-regularized and optimized."
      ],
      "metadata": {
        "id": "r36MBNyPlmjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "  --model model3.py \\\n",
        "  --optimizer \"optim.SGD(model.parameters(), lr=0.03, momentum=0.9,nesterov=True, weight_decay=1e-4)\" \\\n",
        "  --scheduler \"optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=0.0005)\" \\\n",
        "  --train_transforms \"transforms.Compose([transforms.RandomRotation(3), transforms.RandomAffine(0, translate=(0.05, 0.05), shear=5), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\" \\\n",
        "  --test_transforms \"transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbUF63ZalskL",
        "outputId": "79a1984e-ec77-4659-abd7-277d07a5e364"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded model class 'Net' from model3.py\n",
            "CUDA Available? True\n",
            "\n",
            "Download the train and test dataset, with transforms....\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\n",
            "Model Training....\n",
            "\n",
            "Epoch 1\n",
            "Train loss=0.0953 batch_id=468 Accuracy=93.35: 100% 469/469 [00:20<00:00, 23.31it/s]\n",
            "Test set: Average loss: 0.0573, Accuracy: 9827/10000 (98.27%)\n",
            "\n",
            "Epoch 2\n",
            "Train loss=0.0164 batch_id=468 Accuracy=97.98: 100% 469/469 [00:21<00:00, 21.79it/s]\n",
            "Test set: Average loss: 0.0383, Accuracy: 9875/10000 (98.75%)\n",
            "\n",
            "Epoch 3\n",
            "Train loss=0.0280 batch_id=468 Accuracy=98.38: 100% 469/469 [00:21<00:00, 21.72it/s]\n",
            "Test set: Average loss: 0.0275, Accuracy: 9916/10000 (99.16%)\n",
            "\n",
            "Epoch 4\n",
            "Train loss=0.0396 batch_id=468 Accuracy=98.62: 100% 469/469 [00:21<00:00, 22.23it/s]\n",
            "Test set: Average loss: 0.0293, Accuracy: 9902/10000 (99.02%)\n",
            "\n",
            "Epoch 5\n",
            "Train loss=0.0102 batch_id=468 Accuracy=98.75: 100% 469/469 [00:19<00:00, 23.73it/s]\n",
            "Test set: Average loss: 0.0222, Accuracy: 9919/10000 (99.19%)\n",
            "\n",
            "Epoch 6\n",
            "Train loss=0.0113 batch_id=468 Accuracy=98.85: 100% 469/469 [00:21<00:00, 22.01it/s]\n",
            "Test set: Average loss: 0.0256, Accuracy: 9922/10000 (99.22%)\n",
            "\n",
            "Epoch 7\n",
            "Train loss=0.0568 batch_id=468 Accuracy=98.91: 100% 469/469 [00:21<00:00, 22.05it/s]\n",
            "Test set: Average loss: 0.0244, Accuracy: 9917/10000 (99.17%)\n",
            "\n",
            "Epoch 8\n",
            "Train loss=0.0426 batch_id=468 Accuracy=99.00: 100% 469/469 [00:21<00:00, 22.12it/s]\n",
            "Test set: Average loss: 0.0221, Accuracy: 9927/10000 (99.27%)\n",
            "\n",
            "Epoch 9\n",
            "Train loss=0.0631 batch_id=468 Accuracy=99.12: 100% 469/469 [00:20<00:00, 22.88it/s]\n",
            "Test set: Average loss: 0.0195, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "Epoch 10\n",
            "Train loss=0.1005 batch_id=468 Accuracy=99.15: 100% 469/469 [00:20<00:00, 23.25it/s]\n",
            "Test set: Average loss: 0.0205, Accuracy: 9933/10000 (99.33%)\n",
            "\n",
            "Epoch 11\n",
            "Train loss=0.0061 batch_id=468 Accuracy=99.20: 100% 469/469 [00:21<00:00, 21.62it/s]\n",
            "Test set: Average loss: 0.0183, Accuracy: 9941/10000 (99.41%)\n",
            "\n",
            "Epoch 12\n",
            "Train loss=0.0300 batch_id=468 Accuracy=99.28: 100% 469/469 [00:21<00:00, 21.60it/s]\n",
            "Test set: Average loss: 0.0181, Accuracy: 9942/10000 (99.42%)\n",
            "\n",
            "Epoch 13\n",
            "Train loss=0.0048 batch_id=468 Accuracy=99.33: 100% 469/469 [00:20<00:00, 22.34it/s]\n",
            "Test set: Average loss: 0.0183, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "Epoch 14\n",
            "Train loss=0.0475 batch_id=468 Accuracy=99.27: 100% 469/469 [00:20<00:00, 22.97it/s]\n",
            "Test set: Average loss: 0.0178, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "Epoch 15\n",
            "Train loss=0.0140 batch_id=468 Accuracy=99.36: 100% 469/469 [00:21<00:00, 22.21it/s]\n",
            "Test set: Average loss: 0.0180, Accuracy: 9941/10000 (99.41%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-oDRHEwn6RJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}